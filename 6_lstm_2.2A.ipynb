{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train text of 99999000:\t ons anarchists advocate social relations based upon voluntary as...\n",
      "train text of 1000:\t  anarchism originated as a term of abuse first used against earl...\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('train text of {}:\\t {}...'.format(train_size, train_text[:64]))\n",
    "print('train text of {}:\\t {}...'.format(valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    \n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self.text           = text\n",
    "        self.text_size      = len(text)\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_unrollings = num_unrollings\n",
    "\n",
    "        segment        = self.text_size // self.batch_size \n",
    "        self.cursor    = [segment*offset for offset in range(batch_size)]\n",
    "        self.last_batch = self.next_batch()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        batch = []\n",
    "        for b in range(self.batch_size):\n",
    "            char     = self.text[self.cursor[b]]\n",
    "            idx      = char2id(char)\n",
    "            \n",
    "            batch.append(idx)\n",
    "            self.update_cursor(b)\n",
    "\n",
    "        return np.array(batch, dtype=np.int32)\n",
    "    \n",
    "    \n",
    "    def update_cursor(self,b):\n",
    "        self.cursor[b] = (self.cursor[b] + 1) % self.text_size\n",
    "\n",
    "  \n",
    "    def next(self):\n",
    "        batches = [self.last_batch] \n",
    "        for step in range(self.num_unrollings):\n",
    "            batches.append(self.next_batch())\n",
    "        self.last_batch = batches[-1]\n",
    "        \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter    = ord(string.ascii_lowercase[0])\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id2characters(ids):\n",
    "    return [id2char(int(c)) for c in ids]\n",
    "\n",
    "def prob2characters(probabilities):\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches, isprob=False):\n",
    "    characters = prob2characters if isprob else id2characters\n",
    "    \n",
    "    s = [''] * batches[0].shape[0]    \n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    \n",
    "    labels_onehot = []\n",
    "    for label in labels:\n",
    "        letter        = [0 for _ in range(vocabulary_size)]\n",
    "        letter[label] = 1\n",
    "        labels_onehot.append(letter)\n",
    "    \n",
    "    log_prob = np.sum(np.multiply(labels_onehot, -np.log(predictions)))\n",
    "    log_prob = log_prob / labels.shape[0]\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normed probs.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size     = 64\n",
    "num_unrollings = 10\n",
    "num_nodes      = 64\n",
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ists advoca_ate social \n",
      "ary governm_ments faile\n",
      "hes nationa_al park pho\n",
      "d monasteri_ies index s\n",
      "raca prince_ess of cast\n",
      "chard baer _ h provided\n",
      "rgical lang_guage among\n",
      "for passeng_gers in dec\n",
      "the nationa_al media an\n",
      "took place _ during the\n",
      "ther well k_known manuf\n",
      "seven six s_seven a wid\n",
      "ith a gloss_s covering \n",
      "robably bee_en one of t\n",
      "to recogniz_ze single a\n",
      "ceived the _ first card\n",
      "icant than _ in jersey \n",
      "ritic of th_he poverty \n",
      "ight in sig_gns of huma\n",
      "s uncaused _ cause so a\n",
      " lost as in_n denatural\n",
      "cellular ic_ce formatio\n",
      "e size of t_the input u\n",
      " him a stic_ck to pull \n",
      "drugs confu_usion inabi\n",
      " take to co_omplete an \n",
      " the priest_t of the mi\n",
      "im to name _ it fort de\n",
      "d barred at_ttempts by \n",
      "standard fo_ormats for \n",
      " such as es_soteric chr\n",
      "ze on the g_growing pop\n",
      "e of the or_riginal doc\n",
      "d hiver one_e nine eigh\n",
      "y eight mar_rch eight l\n",
      "the lead ch_haracter li\n",
      "es classica_al mechanic\n",
      "ce the non _ gm compari\n",
      "al analysis_s fundament\n",
      "mormons bel_lieve the c\n",
      "t or at lea_ast not par\n",
      " disagreed _ upon by hi\n",
      "ing system _ example rl\n",
      "btypes base_ed on the w\n",
      "anguages th_he official\n",
      "r commissio_on at this \n",
      "ess one nin_ne three tw\n",
      "nux suse li_inux enterp\n",
      " the first _ daily coll\n",
      "zi concentr_ration camp\n",
      " society ne_ehru wished\n",
      "elatively s_stiff from \n",
      "etworks sha_arman s syd\n",
      "or hirohito_o to begin \n",
      "litical ini_itiatives t\n",
      "n most of t_these autho\n",
      "iskerdoo ri_icky ricard\n",
      "ic overview_w of mathem\n",
      "air compone_ent of arm \n",
      "om acnm acc_credited pr\n",
      " centerline_e external \n",
      "e than any _ other stat\n",
      "devotional _ buddhism e\n",
      "de such dev_vices possi\n"
     ]
    }
   ],
   "source": [
    "bs1 = batches2string(train_batches.next())\n",
    "bs2 = batches2string(train_batches.next())\n",
    "\n",
    "for s1,s2 in zip(bs1,bs2):\n",
    "    print('{}_{}'.format(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "problem_2A_graph = tf.Graph()\n",
    "\n",
    "with problem_2A_graph.as_default():  \n",
    "    \n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state  = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # All-in matrixes:\n",
    "    ifoc_x = tf.concat([ix, fx, cx, ox], axis=1) # embed_size x 4*num_nodes\n",
    "    ifoc_m = tf.concat([im, fm, cm, om], axis=1) #  num_nodes x 4*num_nodes\n",
    "    ifoc_b = tf.concat([ib, fb, cb, ob], axis=1) #          1 x 4*num_nodes\n",
    "     \n",
    "    \n",
    "    # Definition of the cell computation. \n",
    "    def lstm_cell(i,o,state):\n",
    "        mmul        = tf.matmul(i, ifoc_x) + tf.matmul(o, ifoc_m) + ifoc_b\n",
    "        im,fm,om,cm = tf.split(mmul, num_or_size_splits=4, axis=1)\n",
    "        \n",
    "        input_gate,forget_gate,output_gate = tf.sigmoid(im),tf.sigmoid(fm),tf.sigmoid(om)\n",
    "        update                             = tf.tanh(cm)\n",
    "       \n",
    "        state       = forget_gate * state + input_gate * update\n",
    "        output      = output_gate * tf.tanh(state)\n",
    "        \n",
    "        return output, state\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    train_shape  = [batch_size]    \n",
    "    train_data   = [tf.placeholder(tf.int32, shape=train_shape) for _ in range(num_unrollings + 1)]    \n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings] # [ :-1]\n",
    "    train_labels = train_data[1:]              # [1: ]\n",
    "\n",
    "    # Embeddings.\n",
    "    embeddings   = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output  = saved_output\n",
    "    state   = saved_state\n",
    "    \n",
    "    for tinput in train_inputs:\n",
    "        embed         = tf.nn.embedding_lookup(embeddings, tinput)\n",
    "        output, state = lstm_cell(embed, output, state)\n",
    "        outputs.append(output)\n",
    "        \n",
    "    \n",
    "    labels  = tf.concat(train_labels, 0)\n",
    "\n",
    "     # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        outputs = tf.concat(outputs, 0)\n",
    "        logits  = tf.nn.xw_plus_b(outputs, w, b)\n",
    "        labels  = tf.concat(train_labels, 0)\n",
    "        labels  = tf.one_hot(labels, vocabulary_size)\n",
    "        loss    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "     # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "     # Optimizer.\n",
    "    global_step   = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer     = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    gradients, v  = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _  = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer     = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "   \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input        = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_embed        = tf.nn.embedding_lookup(embeddings,sample_input)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state  = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state  = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "\n",
    "    sample_output, sample_state = lstm_cell(sample_embed, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.289504 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.83\n",
      "================================================================================\n",
      "byhmow ejejra bly  ve    bvoonmekekt rx  naimmxi wisof xlfpagbnfndtg vlnsa a ppd\n",
      "xi selboeogeljgfemdsststcuhtfhatysznllnyoith nsotrmeqes o lnltrsvexoojedxakrsbsz\n",
      "iziowca nkozx slenalpokm  srpywayes   vssc  qpnzr ped ev ertkator qgtthrhounvjrj\n",
      "dsjsh s iucsc agee vmx ewlbsc ocrietauhl pcnob siminulfnkgm z  jfexgd nm cfhe dr\n",
      "enclwpcun xo oed e uhmvor txpg oesj mksxekylw   tgo qiwrn sncixe nhynydh in vtnl\n",
      "================================================================================\n",
      "Validation set perplexity: 19.46\n",
      "Average loss at step 100: 2.330482 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.10\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 200: 2.013439 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.39\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 300: 1.919182 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 400: 1.852954 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 500: 1.814073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 600: 1.803344 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 700: 1.758716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 800: 1.724721 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.75\n",
      "Average loss at step 900: 1.747527 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1000: 1.745329 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "================================================================================\n",
      "sef the recanidard natroperogft maility it modined and cured their in a filmenti\n",
      " inceltion in hanour nich becan musing lings chiam polist even and four s whilab\n",
      "ing used areassion hiva orgin in tunal verkoris any oftemst infiual directions h\n",
      "e minisle in bejic of the releasionally a tare the dequant only prisoncirevation\n",
      "y of yel had b the now hifs celtion drismatrarism visi d and gazazso in a scaa i\n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 1100: 1.709643 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1200: 1.694225 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1300: 1.675495 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 1400: 1.691419 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 1500: 1.687197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 1600: 1.703036 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 1700: 1.675452 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 1800: 1.646097 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 1900: 1.617286 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2000: 1.660110 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "gratices of tourgent one nine zero zero five five one nine un aldown the statly \n",
      "only finsome in toche br they terpbianaldes greavides to phyip tradision spercit\n",
      "ther deved to rebeveltiona originally to eng stround missiones used but milless \n",
      "des the the and the itside whieurs such the madeatal greis bin phtsoduce his alt\n",
      " the talditizens and forming specired and on the united verony with ironisial re\n",
      "================================================================================\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2100: 1.654492 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2200: 1.653478 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2300: 1.621008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2400: 1.639389 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 2500: 1.667954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2600: 1.637542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.647814 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.639740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 2900: 1.638959 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3000: 1.641810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "zary but tebewhe adhoolument pakent and brastant has h a soccovorban of communit\n",
      "perfobly nine to found recodd bensionant refine countimal quaes of the americati\n",
      "flates in whiter sitiduat opt and bit indated in the game humabods and a besis a\n",
      "quentg one nine six five to taunce attration one nine two one zerormed of chill \n",
      " one computher by include one ni nine zero eight when and numblaritativity he no\n",
      "================================================================================\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 3100: 1.625208 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3200: 1.640762 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3300: 1.636873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3400: 1.665206 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3500: 1.657502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3600: 1.672483 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3700: 1.649599 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 3800: 1.649386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3900: 1.649427 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4000: 1.662603 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "jects of interial occured in alble with number directinnolliow which all hages b\n",
      "ines only withoola infect after the copan with main b amerious of ilonicle nors \n",
      "uses halpss maino advica months after gosthock and noter american bay gyrmage se\n",
      "ting broiffow herr living ok languagulation of the relected two zero six zero ze\n",
      "y to lahre s fars are for their visting a delaums and way kints and reversed to \n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4100: 1.642426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.642792 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4300: 1.626233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 4400: 1.614541 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4500: 1.624416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4600: 1.622133 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4700: 1.635151 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4800: 1.642328 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4900: 1.649416 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.620633 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "y d by typivioy for civiolive day and nonducle is issued ralitation on the romas\n",
      "thing one eight five three zero zero zero five six two two five to two in the si\n",
      "sews of gainity value one in that prodiicd tinus oran of his org traled and the \n",
      "losi definition and s s the dynestanturies tenries one zero zero six simingr the\n",
      "ck good in a poset such does one nine seven cloustinto for secation go good his \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5100: 1.596995 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5200: 1.572028 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 5300: 1.560189 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 5400: 1.557809 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5500: 1.542078 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5600: 1.557153 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5700: 1.544890 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 5800: 1.561372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 5900: 1.549876 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6000: 1.525601 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "fers called three dealticaly nomple system for scat sepis creatics of poneers co\n",
      "uns y autitoration and cahle schono demes in access state only at the delater fo\n",
      "untinue yarklient a zero sixean short of humcwouss the other process typious to \n",
      "ed rederws the signar airchines in the shoulled to otten as a milinam concenting\n",
      "reatal the they in have sox two zero two zero est names the armiaziis large the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6100: 1.542542 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6200: 1.515366 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6300: 1.518687 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "Validation set perplexity: 4.06\n",
      "Average loss at step 6400: 1.517576 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.10\n",
      "Average loss at step 6500: 1.532739 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.11\n",
      "Average loss at step 6600: 1.574546 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.08\n",
      "Average loss at step 6700: 1.559056 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.13\n",
      "Average loss at step 6800: 1.576058 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.12\n",
      "Average loss at step 6900: 1.555555 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.14\n",
      "Average loss at step 7000: 1.549129 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "================================================================================\n",
      "mish per alexandring the chapheer of from mear bars the encorde is concerd in si\n",
      "er of fact beginual com by aborce ho there their was as unichpsilia on e blacker\n",
      "ruth are returniate the in to power header adventure of the are sause varaninger\n",
      "form conomeptice used that of swatertons passation surment as least the artuaste\n",
      "quent from mesona we or pottral bodeta with a metable edsion its migine tranfus \n",
      "================================================================================\n",
      "Validation set perplexity: 4.11\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=problem_2A_graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "    \n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches   = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    for i in range(num_unrollings + 1): \n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    \n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      \n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        \n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        \n",
    "        for _ in range(5):\n",
    "          feed     = sample(random_distribution())\n",
    "          feed     = np.argmax(feed,1)\n",
    "          sentence = id2characters(feed)[0]\n",
    "        \n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed       = sample(prediction)\n",
    "            feed       = np.argmax(feed,1)\n",
    "            \n",
    "            sentence  += id2characters(feed)[0]\n",
    "          print(sentence)\n",
    "        \n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b             = valid_batches.next()\n",
    "        predictions   = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "        \n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
