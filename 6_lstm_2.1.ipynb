{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train text of 99999000:\t ons anarchists advocate social relations based upon voluntary as...\n",
      "train text of 1000:\t  anarchism originated as a term of abuse first used against earl...\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('train text of {}:\\t {}...'.format(train_size, train_text[:64]))\n",
    "print('train text of {}:\\t {}...'.format(valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter    = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "  \n",
    "\n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text       = text\n",
    "        self._text_size  = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment      = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "  \n",
    "    def _next_batch(self):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "  \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "            \n",
    "        self._last_batch = batches[-1]\n",
    "        \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def characters(probabilities):\n",
    "    \"\"\"Turn one-hot encoding or a prob.dist. over chars into (most likely) chars\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Turn seq of one-hot batches into (most likely) string representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]    \n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ons anarchi_ists advoca\n",
      "when milita_ary governm\n",
      "lleria arch_hes nationa\n",
      " abbeys and_d monasteri\n",
      "married urr_raca prince\n",
      "hel and ric_chard baer \n",
      "y and litur_rgical lang\n",
      "ay opened f_for passeng\n",
      "tion from t_the nationa\n",
      "migration t_took place \n",
      "new york ot_ther well k\n",
      "he boeing s_seven six s\n",
      "e listed wi_ith a gloss\n",
      "eber has pr_robably bee\n",
      "o be made t_to recogniz\n",
      "yer who rec_ceived the \n",
      "ore signifi_icant than \n",
      "a fierce cr_ritic of th\n",
      " two six ei_ight in sig\n",
      "aristotle s_s uncaused \n",
      "ity can be _ lost as in\n",
      " and intrac_cellular ic\n",
      "tion of the_e size of t\n",
      "dy to pass _ him a stic\n",
      "f certain d_drugs confu\n",
      "at it will _ take to co\n",
      "e convince _ the priest\n",
      "ent told hi_im to name \n",
      "ampaign and_d barred at\n",
      "rver side s_standard fo\n",
      "ious texts _ such as es\n",
      "o capitaliz_ze on the g\n",
      "a duplicate_e of the or\n",
      "gh ann es d_d hiver one\n",
      "ine january_y eight mar\n",
      "ross zero t_the lead ch\n",
      "cal theorie_es classica\n",
      "ast instanc_ce the non \n",
      " dimensiona_al analysis\n",
      "most holy m_mormons bel\n",
      "t s support_t or at lea\n",
      "u is still _ disagreed \n",
      "e oscillati_ing system \n",
      "o eight sub_btypes base\n",
      "of italy la_anguages th\n",
      "s the tower_r commissio\n",
      "klahoma pre_ess one nin\n",
      "erprise lin_nux suse li\n",
      "ws becomes _ the first \n",
      "et in a naz_zi concentr\n",
      "the fabian _ society ne\n",
      "etchy to re_elatively s\n",
      " sharman ne_etworks sha\n",
      "ised empero_or hirohito\n",
      "ting in pol_litical ini\n",
      "d neo latin_n most of t\n",
      "th risky ri_iskerdoo ri\n",
      "encyclopedi_ic overview\n",
      "fense the a_air compone\n",
      "duating fro_om acnm acc\n",
      "treet grid _ centerline\n",
      "ations more_e than any \n",
      "appeal of d_devotional \n",
      "si have mad_de such dev\n"
     ]
    }
   ],
   "source": [
    "bs1 = batches2string(train_batches.next())\n",
    "bs2 = batches2string(train_batches.next())\n",
    "\n",
    "for s1,s2 in zip(bs1,bs2):\n",
    "    print('{}_{}'.format(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    \n",
    "    log_prob = np.sum(np.multiply(labels, -np.log(predictions)))\n",
    "    log_prob = log_prob / labels.shape[0]\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normed probs.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "problem1_graph = tf.Graph()\n",
    "with problem1_graph.as_default():  \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    # Memory cell: input, state and bias.                             \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state  = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # All-in matrix:\n",
    "    ifoc_x = tf.concat([ix, fx, cx, ox], axis=1) # vocab_size x 4*num_nodes\n",
    "    ifoc_m = tf.concat([im, fm, cm, om], axis=1) #  num_nodes x 4*num_nodes\n",
    "    ifoc_b = tf.concat([ib, fb, cb, ob], axis=1) #          1 x 4*num_nodes\n",
    "     \n",
    "    \n",
    "    # Definition of the cell computation. \n",
    "    '''\n",
    "    Alternative:\n",
    "    def lstm_cell(i,o, state):\n",
    "        mmul = tf.matmul(i, ifoc_x) + tf.matmul(o, ifoc_m) + ifoc_b # 1 x 4*num_nodes\n",
    "        sig  = tf.sigmoid(mmul)                                     \n",
    "        tan  = tf.tanh(mmul)\n",
    "        \n",
    "        input_gate,forget_gate,output_gate,_ = tf.split(sig, num_or_size_splits=4, axis=1) # (1 x num_nodes) x 4\n",
    "        _,_,_,update                         = tf.split(tan, num_or_size_splits=4, axis=1) # (1 x num_nodes) x 4\n",
    "        \n",
    "        state       = forget_gate * state + input_gate * update\n",
    "        output      = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "    '''\n",
    "    def lstm_cell(i,o, state):\n",
    "        mmul        = tf.matmul(i, ifoc_x) + tf.matmul(o, ifoc_m) + ifoc_b # 1 x 4*num_nodes\n",
    "        im,fm,om,cm = tf.split(mmul, num_or_size_splits=4, axis=1) # (1 x num_nodes) x 4\n",
    "        \n",
    "        input_gate,forget_gate,output_gate = tf.sigmoid(im),tf.sigmoid(fm),tf.sigmoid(om)\n",
    "        update                             = tf.tanh(cm)\n",
    "       \n",
    "        state       = forget_gate * state + input_gate * update\n",
    "        output      = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "\n",
    "    \n",
    "    # Input data.\n",
    "    train_shape  = [batch_size,vocabulary_size]    \n",
    "    train_data   = [tf.placeholder(tf.float32, shape=train_shape) for _ in range(num_unrollings + 1)]    \n",
    "    \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output  = saved_output\n",
    "    state   = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "        \n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        labels = tf.concat(train_labels, 0)\n",
    "        loss   = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step   = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer     = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    gradients, v  = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _  = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer     = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input        = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state  = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state  = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294038 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "tlbspa lht  ila nfagko moebrjkgjwlwrpenzjtfzpeanle tuxitnxpbfegvwxtkiytbeehl izo\n",
      "umae emimi cfmsfoefeghf sxp ddooror cwgwj mybqstabrha  p jcdieobcr hlsctctmeckcf\n",
      "so faxv ctqr n osk ouddpalbttck afdzaoe khngvs bmo mbhirbjw ryeah qqvxtdeedpkein\n",
      "asyactiumwrgnewpmgv al thilnsnapqeeqa bi whciobwb eemassaunpne hs  wd g  or hwoo\n",
      " zonnadzgxn y ovttxzbewaajlniikpumn ir alstc   ltviiy  wiu j fxq ttvtjiaopjswt  \n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.586895 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.24\n",
      "Validation set perplexity: 10.55\n",
      "Average loss at step 200: 2.250635 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.60\n",
      "Validation set perplexity: 8.75\n",
      "Average loss at step 300: 2.100298 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.52\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 400: 2.003949 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.53\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 500: 1.937087 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.21\n",
      "Average loss at step 600: 1.908987 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 7.19\n",
      "Average loss at step 700: 1.860284 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 800: 1.818400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.830479 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Validation set perplexity: 6.30\n",
      "Average loss at step 1000: 1.823442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "================================================================================\n",
      "n a nive aroup hoovfle warier wind reffor nine feve elawe is poner nd condally p\n",
      "reldor age exilust a hid desublic subsilaisovich for imass on a ilvea d spin lev\n",
      "nies the center and one lectrerion ymece wadusts speided to the ternour vivalish\n",
      "inal of initvis bredso phassia dist vea yeace adweally a beed the one nine one f\n",
      "umity four noul of the beli one one nive arsignto to yolal offers one nine poper\n",
      "================================================================================\n",
      "Validation set perplexity: 6.19\n",
      "Average loss at step 1100: 1.778999 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.95\n",
      "Average loss at step 1200: 1.755241 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1300: 1.733984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1400: 1.745393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.67\n",
      "Average loss at step 1500: 1.736988 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.750712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 1700: 1.712620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 1800: 1.673185 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1900: 1.645692 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2000: 1.695207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "quia sense is a producty pond produterz land sitferal browing to his fualeb gapl\n",
      "c ilbingle saigh in citest bu van whethndims moviltive badd is resulnse had kift\n",
      "ky appuce and de comedon as wheren the spanfly thus that diadaic of accued in tu\n",
      "zeanls indist di the cooled actie for hughis quzked of cational low i west of no\n",
      "jording water about to the wibll dean the bandby one nine that from links vanyon\n",
      "================================================================================\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 2100: 1.684316 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2200: 1.682677 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2300: 1.641797 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2400: 1.662481 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 2500: 1.679348 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2600: 1.654476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2700: 1.656753 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2800: 1.652789 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 2900: 1.650275 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3000: 1.652249 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "ders and syde subsuip during bocutive light and scourequ afptope of is mp delati\n",
      "ks of shy in the enember punces in succersis deusedaralathorst and of shota armi\n",
      " alintion libal one nine six nine one zero five varuses a chrest parkilion will \n",
      "viviss agorisand became than invitutive areugled the files and fichior biern ele\n",
      "e one nalley heap of and wrike as gainish othen the femilial american punpeffonc\n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3100: 1.629619 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3200: 1.645632 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3300: 1.635299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 3400: 1.670986 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 3500: 1.655638 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.666905 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 3700: 1.644468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 3800: 1.642351 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3900: 1.635149 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4000: 1.649428 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "jy mere hatal was easfies having noy is the used the germated attiendia the it t\n",
      "quited rs an for it rees the englinicat levedland or dual ond interested with fo\n",
      "be piepare with bass also perfossiate gremabellyrrion diast to bitaried in revel\n",
      "emplimer in histeaska gendy and gurons to itsertald hundocration ip was three on\n",
      "lo schodoo khwe his sal dyome of heraph soceed the cays hockari his discet leus \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.631953 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4200: 1.631815 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4300: 1.616783 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4400: 1.609804 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4500: 1.612541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4600: 1.612442 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4700: 1.627709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4800: 1.628911 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 4900: 1.634322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.606747 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "================================================================================\n",
      "couse often rimines he ropa allowed who batthketion renast one five eight two si\n",
      "d be schilmes sley age playeols assulate in these fasth be lovate veased or ope \n",
      "tion it one bond the moves despspre seen are not not hiscerfica e and dials ther\n",
      "jublin did rescamelones in the field s mollematic t detecter e joes jukues seate\n",
      "cater favion earting will over nine gahgurzach one one gincinds bon smante audin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 5100: 1.602322 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 5200: 1.594831 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.573852 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.577450 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.564244 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5600: 1.578512 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.569032 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.578980 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5900: 1.570469 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.548261 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "y mosh s recaltoppboll  stanlers of the france lists one nine nine one eight one\n",
      "way migal one nine eigle fillars to a adbitchonia ground for the form the mese d\n",
      "menicres but feet hels eas only have the tours colspince delawes as conmons hish\n",
      "gionly musikashling to such le dispute adaing his stords in wyrelean yeasions al\n",
      "note sheftle film this south ambi us of power the ameiso reference on ompero to \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6100: 1.562923 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6200: 1.531395 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.542190 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.539098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.552414 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.592173 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6700: 1.578591 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6800: 1.602572 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6900: 1.578037 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 7000: 1.575430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "bly armeopto considered this become one nine five one nine five christle here co\n",
      "zard ware where one nine six zero three four ny word vertically slow they anjus \n",
      "ba the lawbly weld of the patmite and field and nine of recent for iment north m\n",
      "jues on the not come electric modher is twi bu on twi million is carru service r\n",
      "zs as life of the elynable sometimi amfin notebriting the indice of expeelo to f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=problem1_graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
