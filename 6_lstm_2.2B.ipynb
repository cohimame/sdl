{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import collections\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:150]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train text of 99999000:\t ons anarchists advocate social relations based upon voluntary as...\n",
      "train text of 1000:\t  anarchism originated as a term of abuse first used against earl...\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print('train text of {}:\\t {}...'.format(train_size, train_text[:64]))\n",
    "print('train text of {}:\\t {}...'.format(valid_size, valid_text[:64]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2.B,C\n",
    "---------\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self.text           = text\n",
    "        self.text_size      = len(text)\n",
    "        self.batch_size     = batch_size\n",
    "        self.num_unrollings = num_unrollings\n",
    "        segment             = self.text_size // self.batch_size \n",
    "        self.cursor         = [segment*offset for offset in range(batch_size)]\n",
    "        self.last_batch     = self.next_batch()\n",
    "        \n",
    "    def next_batch(self):\n",
    "        batch = []\n",
    "        for b in range(self.batch_size):\n",
    "            char     = self.text[self.cursor[b]]\n",
    "            idx      = char2id(char)\n",
    "            batch.append(idx)\n",
    "            self.update_cursor(b)\n",
    "        return np.array(batch, dtype=np.int32)\n",
    "    \n",
    "    def update_cursor(self,b):\n",
    "        self.cursor[b] = (self.cursor[b] + 1) % self.text_size\n",
    "\n",
    "    def next(self):\n",
    "        batches = [self.last_batch] \n",
    "        for step in range(self.num_unrollings):\n",
    "            batches.append(self.next_batch())\n",
    "        self.last_batch = batches[-1]        \n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 729 128\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size   = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter      = ord(string.ascii_lowercase[0])\n",
    "embedding_size    = 128\n",
    "bigram_vocab_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "print(vocabulary_size, bigram_vocab_size, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: Ã¯\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    else:\n",
    "        if char != ' ':\n",
    "            print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "def id2char(dictid): \n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('Ã¯'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "double_index = [(i,j) for i in range(vocabulary_size) for j in range(vocabulary_size)]\n",
    "double_index = {bigram:i for i,bigram in enumerate(double_index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: Ã¯\n",
      "0 1 703\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    double_index[(char2id(' '), char2id('Ã¯'))],\n",
    "    double_index[(char2id(' '), char2id('a'))],\n",
    "    double_index[(char2id('z'), char2id('a'))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def id2characters(ids):             return [id2char(int(c)) for c in ids]\n",
    "def prob2characters(probabilities): return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "def batches2string(batches, isprob=False):\n",
    "    characters = prob2characters if isprob else id2characters\n",
    "    s = [''] * batches[0].shape[0]    \n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size     = 64\n",
    "num_unrollings = 10\n",
    "num_nodes      = 64\n",
    "embedding_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batches.next()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ists advoca_ate social \n",
      "ary governm_ments faile\n",
      "hes nationa_al park pho\n",
      "d monasteri_ies index s\n",
      "raca prince_ess of cast\n",
      "chard baer _ h provided\n",
      "rgical lang_guage among\n",
      "for passeng_gers in dec\n",
      "the nationa_al media an\n",
      "took place _ during the\n",
      "ther well k_known manuf\n",
      "seven six s_seven a wid\n",
      "ith a gloss_s covering \n",
      "robably bee_en one of t\n",
      "to recogniz_ze single a\n",
      "ceived the _ first card\n",
      "icant than _ in jersey \n",
      "ritic of th_he poverty \n",
      "ight in sig_gns of huma\n",
      "s uncaused _ cause so a\n",
      " lost as in_n denatural\n",
      "cellular ic_ce formatio\n",
      "e size of t_the input u\n",
      " him a stic_ck to pull \n",
      "drugs confu_usion inabi\n",
      " take to co_omplete an \n",
      " the priest_t of the mi\n",
      "im to name _ it fort de\n",
      "d barred at_ttempts by \n",
      "standard fo_ormats for \n",
      " such as es_soteric chr\n",
      "ze on the g_growing pop\n",
      "e of the or_riginal doc\n",
      "d hiver one_e nine eigh\n",
      "y eight mar_rch eight l\n",
      "the lead ch_haracter li\n",
      "es classica_al mechanic\n",
      "ce the non _ gm compari\n",
      "al analysis_s fundament\n",
      "mormons bel_lieve the c\n",
      "t or at lea_ast not par\n",
      " disagreed _ upon by hi\n",
      "ing system _ example rl\n",
      "btypes base_ed on the w\n",
      "anguages th_he official\n",
      "r commissio_on at this \n",
      "ess one nin_ne three tw\n",
      "nux suse li_inux enterp\n",
      " the first _ daily coll\n",
      "zi concentr_ration camp\n",
      " society ne_ehru wished\n",
      "elatively s_stiff from \n",
      "etworks sha_arman s syd\n",
      "or hirohito_o to begin \n",
      "litical ini_itiatives t\n",
      "n most of t_these autho\n",
      "iskerdoo ri_icky ricard\n",
      "ic overview_w of mathem\n",
      "air compone_ent of arm \n",
      "om acnm acc_credited pr\n",
      " centerline_e external \n",
      "e than any _ other stat\n",
      "devotional _ buddhism e\n",
      "de such dev_vices possi\n"
     ]
    }
   ],
   "source": [
    "bs1 = batches2string(train_batches.next())\n",
    "bs2 = batches2string(train_batches.next())\n",
    "\n",
    "for s1,s2 in zip(bs1,bs2):\n",
    "    print('{}_{}'.format(s1,s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    labels_onehot = []\n",
    "    for label in labels:\n",
    "        letter        = [0 for _ in range(vocabulary_size)]\n",
    "        letter[label] = 1\n",
    "        labels_onehot.append(letter)\n",
    "    log_prob = np.sum(np.multiply(labels_onehot, -np.log(predictions)))\n",
    "    log_prob = log_prob / labels.shape[0]\n",
    "    return log_prob\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normed probs.\"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_2B_graph = tf.Graph()\n",
    "\n",
    "with problem_2B_graph.as_default():  \n",
    "    \n",
    "    ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    ifoc_x = tf.concat([ix, fx, cx, ox], axis=1) # embed_size x 4*num_nodes\n",
    "    ifoc_m = tf.concat([im, fm, cm, om], axis=1) #  num_nodes x 4*num_nodes\n",
    "    ifoc_b = tf.concat([ib, fb, cb, ob], axis=1) #          1 x 4*num_nodes\n",
    "     \n",
    "    def lstm_cell(i,o,state):\n",
    "        mmul        = tf.matmul(i, ifoc_x) + tf.matmul(o, ifoc_m) + ifoc_b\n",
    "        im,fm,om,cm = tf.split(mmul, num_or_size_splits=4, axis=1)\n",
    "        input_gate,forget_gate,output_gate = tf.sigmoid(im),tf.sigmoid(fm),tf.sigmoid(om)\n",
    "        update                             = tf.tanh(cm)\n",
    "        state       = forget_gate * state + input_gate * update\n",
    "        output      = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "\n",
    "    \n",
    "     # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state  = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "    # Classifier weights and biases: we still predict only one next character\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Embeddings.\n",
    "    bigram_embeddings = tf.Variable(tf.random_uniform([bigram_vocab_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "    # Input data. \n",
    "    train_data   = [tf.placeholder(tf.int32, shape=(batch_size)) for _ in range(num_unrollings + 1)]        \n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_inputs = zip(train_inputs[:-1],train_inputs[1:])\n",
    "    train_labels = train_data[2:]\n",
    "\n",
    "    \n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output  = saved_output\n",
    "    state   = saved_state\n",
    "    \n",
    "    for tinput in train_inputs:\n",
    "        tinput        = tinput[0] + vocabulary_size * tinput[1] # (id1,id2) -> id[1,2]\n",
    "        embed         = tf.nn.embedding_lookup(bigram_embeddings, tinput)\n",
    "        output, state = lstm_cell(embed, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    labels  = tf.concat(train_labels,0)\n",
    "    outputs = tf.concat(outputs, 0)\n",
    "    \n",
    "    \n",
    "     # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):    \n",
    "        # Classifier.\n",
    "        outputs = tf.nn.dropout(outputs,keep_prob=0.7)\n",
    "        logits  = tf.nn.xw_plus_b(outputs, w, b)\n",
    "        labels  = tf.concat(train_labels, 0)\n",
    "        labels  = tf.one_hot(labels, vocabulary_size)\n",
    "        loss    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "     # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "     # Optimizer.\n",
    "    global_step   = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer     = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    \n",
    "    gradients, v  = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _  = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer     = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Sampling and validation eval.\n",
    "    sample_input        = [tf.placeholder(tf.int32, shape=[1]) for _ in range(2)]\n",
    "    sample_bigram_id    = sample_input[0] + vocabulary_size * sample_input[1]\n",
    "    sample_embed        = tf.nn.embedding_lookup(bigram_embeddings, sample_bigram_id)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state  = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "    reset_sample_state  = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "\n",
    "    sample_output, sample_state = lstm_cell(sample_embed, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        \n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.308532 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.34\n",
      "================================================================================\n",
      "tt eymeywenxatmzhgpiddemrigotstkvtolkei e exesxoazzxnnpwnfd haee  xcokcg  rdd lpk\n",
      "qna ab  dxkua ljwijp fbe z m cihmdmn  oen goci s onwaosir gzd  rectsa pes dmnrq e\n",
      "kicbowmt oav s ylammjeyjbzehte iaw iqeoo arsgci h cpz bk naih fgzuwv eztwzbrrnhub\n",
      "kulhdmvl nheggrqqghh acrof iw zewkr o aoihwehlu daysholvu e zeuwxybaifjstx zks w \n",
      "aueercslteyif ueioe zreilnibge c vipegaeagggnhqhusmbnudeyidkopeino rskumejpirvxbt\n",
      "================================================================================\n",
      "Validation set perplexity: 19.75\n",
      "Average loss at step 100: 2.416801 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 9.66\n",
      "Average loss at step 200: 2.111246 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.75\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 300: 2.031928 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 8.50\n",
      "Average loss at step 400: 1.996088 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.74\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 500: 1.973011 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 600: 1.913440 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 700: 1.897529 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 800: 1.913519 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 900: 1.895870 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.11\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 1000: 1.906992 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "================================================================================\n",
      "gwal be  one nine five one two kmnoh this thas a pust by reition dunce in that a \n",
      "  eight one neone eight with chinary two six of thpact qoht in of palcian one two\n",
      "ues of into sistritiags two two zero zero zero one nine eight ing by hcwt two x a\n",
      "sxim furi brahe ensia proves h one eight whoni zero john munibardi is of jehonal \n",
      "mmus a parme one nover ne nine nine guing and earty gerts some from upraction wit\n",
      "================================================================================\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 1100: 1.872867 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.94\n",
      "Average loss at step 1200: 1.851366 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.74\n",
      "Average loss at step 1300: 1.845784 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 7.85\n",
      "Average loss at step 1400: 1.859807 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 1500: 1.846672 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 1600: 1.837971 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.64\n",
      "Validation set perplexity: 7.65\n",
      "Average loss at step 1700: 1.823156 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 1800: 1.805665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 1900: 1.813103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 2000: 1.806692 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "================================================================================\n",
      "hkty instor one the him inform mode not of of the memihas defendents concort stan\n",
      "xebank eigree fine nine nine also swegue allied cospace gtcan recies of topanable\n",
      "ikiguropy deficallod ex lifecalb misniaut theloper four eight two fitznentome at \n",
      "yuus pius of gall albecitaly procell cout he daan gazill re pricaked issually the\n",
      "yhi greatendenes isre lamments there camentally desley exputions with thered incl\n",
      "================================================================================\n",
      "Validation set perplexity: 7.87\n",
      "Average loss at step 2100: 1.821469 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 2200: 1.827763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 7.62\n",
      "Average loss at step 2300: 1.838597 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 2400: 1.817337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.71\n",
      "Average loss at step 2500: 1.820658 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 2600: 1.805976 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 7.64\n",
      "Average loss at step 2700: 1.818824 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.75\n",
      "Average loss at step 2800: 1.821092 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 2900: 1.813533 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.16\n",
      "Validation set perplexity: 7.97\n",
      "Average loss at step 3000: 1.820065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "================================================================================\n",
      "cherned bapy for poscescrhly play dates the two zero zero two withe secrilasted i\n",
      "fs the enose of drrents qiked and there the coullirlasold in gencts barthar israo\n",
      "gn the tent the shoryfus the damrib knairsience sent aelalrucs ny inectudous hera\n",
      "bby one eight againzs of the use by turds memode reen cent cipfds the the ergy is\n",
      "bver contrustics frembham frequestal see eopuvies one nine syrouth the actermany \n",
      "================================================================================\n",
      "Validation set perplexity: 7.80\n",
      "Average loss at step 3100: 1.802528 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 8.01\n",
      "Average loss at step 3200: 1.780375 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 3300: 1.794765 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.38\n",
      "Validation set perplexity: 7.60\n",
      "Average loss at step 3400: 1.781840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 3500: 1.817180 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 3600: 1.801211 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 3700: 1.795083 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 3800: 1.803880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 7.57\n",
      "Average loss at step 3900: 1.804533 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 4000: 1.799556 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "================================================================================\n",
      "zto rekeuson neburations one six six grelu forst to when one seven two nine two u\n",
      "riting is inguasia all will th for for in and list one in signations b one nine s\n",
      "df the bovere ruls on rainish one six two sing white and one six zero zero zero o\n",
      "iqua thn lobad strictal usaki and siven inrepnsible etter stry and and in the and\n",
      "bpers hick unnot mommir her between three wich in in time acower on zero a finant\n",
      "================================================================================\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 4100: 1.783254 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 4200: 1.770589 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 7.63\n",
      "Average loss at step 4300: 1.776569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.59\n",
      "Average loss at step 4400: 1.767438 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.79\n",
      "Average loss at step 4500: 1.802774 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.78\n",
      "Average loss at step 4600: 1.781706 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.75\n",
      "Validation set perplexity: 7.67\n",
      "Average loss at step 4700: 1.781015 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.91\n",
      "Average loss at step 4800: 1.777423 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.95\n",
      "Average loss at step 4900: 1.788936 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 5000: 1.774823 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "================================================================================\n",
      "pkgne city in the singes three two one one nine with is the two glow such the dia\n",
      "ey ininglion gears for tof the cractificifies of the  ever in the leassm the one \n",
      "was the englid clandshinesf found most at reposonaltose one two zero five eight t\n",
      " imples and the as of and the vtpups a three two three six one five was american \n",
      "systets kotting and intyl lack implise chsobasten foroughtnation an a ced specian\n",
      "================================================================================\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 5100: 1.755758 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 7.66\n",
      "Average loss at step 5200: 1.747905 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 7.47\n",
      "Average loss at step 5300: 1.747992 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 5400: 1.750382 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.45\n",
      "Average loss at step 5500: 1.744936 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 5600: 1.716231 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 5700: 1.726723 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 5800: 1.746846 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 5900: 1.729880 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6000: 1.730358 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "================================================================================\n",
      "vll chinsgement as guerime often in general a camburg is ward one vofive military\n",
      "use note first assing has wells the all to univoight for english approcial delate\n",
      "mtere and masary to one nine nine nine four miane part from the from the desentiw\n",
      "skalssiwunks to city the tqdtm for hawn phyn repuular states the more depenst his\n",
      " guite mikittevanhal caven algo villigunstron of the some and on the sabtal sland\n",
      "================================================================================\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 6100: 1.729467 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 6200: 1.736538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 6300: 1.739238 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.97\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 6400: 1.725508 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.49\n",
      "Average loss at step 6500: 1.701844 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.58\n",
      "Average loss at step 6600: 1.752899 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Validation set perplexity: 7.54\n",
      "Average loss at step 6700: 1.722817 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.50\n",
      "Average loss at step 6800: 1.730700 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.51\n",
      "Average loss at step 6900: 1.722616 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.46\n",
      "Average loss at step 7000: 1.742575 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.89\n",
      "================================================================================\n",
      "oke ccun lokklrity bit many of hum and ons of the over survicobn vegiv supbviging\n",
      "tkbe would nestly in suppollogy without ther hah sociously reour to uan catraholi\n",
      "mundess composito latamed to the number al sious and manion to a buchy jlaba knhy\n",
      "my the bovids the s populanan with thomel own adding sobion of one nine and a nam\n",
      "zlwome experisted to they the fichishbargititify leard two seven are in carement \n",
      "================================================================================\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 7100: 1.744293 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.17\n",
      "Validation set perplexity: 7.44\n",
      "Average loss at step 7200: 1.716249 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 7300: 1.704363 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 7400: 1.710432 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 7500: 1.747457 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 7600: 1.720819 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 7700: 1.723413 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 7800: 1.739690 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 7900: 1.748624 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.43\n",
      "Average loss at step 8000: 1.686040 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "rman prokestire webss in bepace reachrialisn piianment th cam in to a runsh colem\n",
      "lken dinholtic klingonsian alection struessitic plan occests in difficestury ina \n",
      "less wesd querress residence caic and irish a de glander ig life moije under high\n",
      "gdous phave new form and soce agreaty kir and labe of new after one nine seven li\n",
      "zh a four the thae unite of in of tho of poses is frazaliy from the book volbut m\n",
      "================================================================================\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 8100: 1.684634 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 8200: 1.728950 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8300: 1.710834 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8400: 1.704031 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 8500: 1.702719 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 8600: 1.712724 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 8700: 1.751395 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 8800: 1.708880 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 8900: 1.716127 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9000: 1.738041 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "ozine lier beng the mailwloe and one nine eight bikie the tins eleceates all rjli\n",
      "bgas on soat myu tobfying other evire of commoscopolase before in to logy for nic\n",
      "g enecter was to jss sark triboting imputer illing handhuwsef offectmore as mustu\n",
      "july sinces one four actistic creature everally of the which that octom s numbers\n",
      "pbcie setal in been argumi on noms of bas and ver is number that eight borda time\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 7.35\n",
      "Average loss at step 9100: 1.726708 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 9200: 1.706585 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 9300: 1.720121 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 7.39\n",
      "Average loss at step 9400: 1.721768 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.66\n",
      "Validation set perplexity: 7.42\n",
      "Average loss at step 9500: 1.754011 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 9600: 1.729823 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 9700: 1.746186 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 9800: 1.712619 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 9900: 1.718312 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 7.40\n",
      "Average loss at step 10000: 1.727012 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.85\n",
      "================================================================================\n",
      "ozsa isprourchen alescon one or eerly of get be e commoutions alturnish of had pu\n",
      "ike count of filem intersubury counts and the imple in bashnor hagrament of the m\n",
      "bzobbh is army the subye lice fimapla atroms to catrouslaw march in the kead air \n",
      "b one four seven two th klahnia to tourications specrforees and columeisr output \n",
      " yoint simuts imported on the eveb be gard of the more was frequence to benticorg\n",
      "================================================================================\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 10100: 1.728790 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 10200: 1.728448 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 10300: 1.712633 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 10400: 1.720478 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 10500: 1.718435 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.68\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 10600: 1.720385 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 10700: 1.724482 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 10800: 1.722959 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 10900: 1.712977 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 11000: 1.723376 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.45\n",
      "================================================================================\n",
      "zft is evident of pannes of nabor to of is savoure most to see in the graman sip \n",
      "nism alf of can the i differal edition life in mise sem republic one nine nine ei\n",
      "ufifesone or two ont distateufater of val eqrttack cheasurually and public and gr\n",
      "succe deleing the usuiling with over peoplam of h a gropulao the rampary fins tha\n",
      "wdarmer incioths usurbeen be ges one nine nine six and chiles city to in one nine\n",
      "================================================================================\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 11100: 1.722239 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.17\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 11200: 1.717486 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 11300: 1.702825 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 11400: 1.722112 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 11500: 1.736771 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 11600: 1.740553 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 11700: 1.716281 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 11800: 1.707225 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 11900: 1.724398 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.37\n",
      "Average loss at step 12000: 1.740299 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.91\n",
      "================================================================================\n",
      "ny there sa isdtyimase a rufranssuverzer dics former califer eight seven minisbur\n",
      "ythes the commerming addrivexise of the an ernic distansinnist two almost mades o\n",
      "lvuolization them primilishible caus rated by on ner of these in and decisten ope\n",
      "khand mennessial endie distivines his ctions germed saburg at one three one five \n",
      "xed the wearcic expirmed radly rrihim exmert his which and namosystimulation be m\n",
      "================================================================================\n",
      "Validation set perplexity: 7.36\n",
      "Average loss at step 12100: 1.769983 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12200: 1.767789 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12300: 1.737799 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 12400: 1.736283 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12500: 1.718690 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12600: 1.751770 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 12700: 1.718435 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 7.35\n",
      "Average loss at step 12800: 1.712905 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 12900: 1.739345 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 13000: 1.709394 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "ay wher overal that also such of dafleflohoic tably instacire self a poly preside\n",
      "wfits penation the scostony is mude compani during sciates from kion of extenskoa\n",
      "zn the monks intenihetsycliqn parking the it the orthing rockoslawars official h \n",
      "eptifications greas one two with and delevel inscould get system eatters thead an\n",
      "rwildaned ho later two five and gh it holds corm as for the is nigued by phaaces \n",
      "================================================================================\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 13100: 1.724978 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 13200: 1.752748 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 13300: 1.744222 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 13400: 1.778352 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.34\n",
      "Average loss at step 13500: 1.772124 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 13600: 1.739539 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 13700: 1.730535 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 13800: 1.717850 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 13900: 1.708914 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 14000: 1.716808 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.42\n",
      "================================================================================\n",
      "gjlippets stile when there modern irotest amongual trals curch branzeranly who is\n",
      " united three the controtocal pundas breswyed as infor of mal cell to challed dis\n",
      "rvinion micround is heavation mear understived the ending noom garch caples is ca\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uject is christs zero sible two nine eight in capitack olding the pointy the comp\n",
      "wkrty of for gerege not b foppose tegreose as that olitan yuccoped two three in t\n",
      "================================================================================\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 14100: 1.698205 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 14200: 1.726408 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 14300: 1.734005 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 14400: 1.723041 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 14500: 1.700109 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 14600: 1.722695 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 14700: 1.723665 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 14800: 1.722763 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 14900: 1.726332 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 15000: 1.707236 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "ole ping for s discrithosm a fract clit also hightunally song caderatocattorous a\n",
      "mf the centh the altwo and sinter owcfc cration was asselounder pe posititus preq\n",
      "xslop c chankin those irestored lociencee thigust on softworhad a cobeuction berg\n",
      "vs matht varial tarss rown in desteuding mentt of reviside extent high leak north\n",
      "zegent of godist end of is the p committe for vieward but aaled throughtester and\n",
      "================================================================================\n",
      "Validation set perplexity: 7.33\n",
      "Average loss at step 15100: 1.724305 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 15200: 1.722973 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 15300: 1.734158 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 15400: 1.716934 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.32\n",
      "Average loss at step 15500: 1.722909 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 15600: 1.704466 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 15700: 1.721892 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 15800: 1.746386 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 15900: 1.730347 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 16000: 1.738377 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.05\n",
      "================================================================================\n",
      "mxos onlysing example the ability his and there of all secular the toom page form\n",
      " x in there ark by less karlatory compleats speould it finamentt surboren bardesm\n",
      "nvaristed a necles there dent of the respanitively the crostropeo however piora m\n",
      "qrage farouthbtal informatey not to activiced to and funnemirash seven and also s\n",
      "ajamaventek danause other esh difficial stirlster one nine eight israte and the m\n",
      "================================================================================\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 16100: 1.736810 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 16200: 1.750050 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 16300: 1.735170 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 16400: 1.724850 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 16500: 1.750098 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 16600: 1.738872 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 16700: 1.730001 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 16800: 1.732241 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 16900: 1.730908 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17000: 1.751835 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.96\n",
      "================================================================================\n",
      "shehoignankd has the glogianiza her berlatimety criticists indies in the as and l\n",
      "pk nor a rieg white the titions in one three nine cadough containe hams ital dric\n",
      "een are um as attempandarding seven people giction just through the g ess its fro\n",
      "hnian linchs has very among the a proad with own of user selopmely by one three w\n",
      "j il bajtary heres for mernate ved revhures aards to plap gustnt wheths it p extr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17100: 1.725511 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17200: 1.738239 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17300: 1.737249 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17400: 1.736034 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17500: 1.716937 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17600: 1.736467 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17700: 1.709205 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17800: 1.709443 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 17900: 1.705330 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18000: 1.727715 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "ivia were prosr refect it dobide tit the that under in swesed in four john ring o\n",
      "qhs ard all the musight of made who in hovering theera will two thed and ical rad\n",
      "econtnecer inside galianets discomement of the scallesasty onmited one six kiscpz\n",
      "bals alouidf posing on pobe kind theehorded augu as hrouse rochip unibutural citi\n",
      "bong that cu pecillizations had to to processian seriendia the scrigher pioa as o\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18100: 1.730848 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18200: 1.715025 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18300: 1.683242 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18400: 1.709127 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 18500: 1.705084 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18600: 1.706653 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 18700: 1.719027 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.60\n",
      "Validation set perplexity: 7.29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 18800: 1.721345 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 18900: 1.748432 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 19000: 1.746819 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "xv rate sate in the blaked nine nine five five stard way diseate her with aael al\n",
      "ed that relian his first see by the cight one nine eight two naturing metfour hou\n",
      "wern fickcumer was aare imple zero zero five mand marial craffi take actively exc\n",
      "gger cutd heb natorian sector asdcathmany foundition frame tolodle disbiality tea\n",
      "ehyste mause sunwtktow of simple in who the proburs in the francb with lither man\n",
      "================================================================================\n",
      "Validation set perplexity: 7.29\n",
      "Average loss at step 19100: 1.729191 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19200: 1.743451 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19300: 1.718885 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19400: 1.710336 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19500: 1.701986 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19600: 1.699391 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19700: 1.737685 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19800: 1.722463 learning rate: 0.010000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 19900: 1.719116 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 20000: 1.684692 learning rate: 0.001000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "ms merxes grala from both a symber as the xeste sistes own archarmy he tracted no\n",
      "qst in the gore brifish as teepying a egytoley mannelat le mistanalue he souchas \n",
      "hvd hallown has aclock to ragicase miredi two zero zero zero eight eight four gav\n",
      "pmarascosfellically pare releatat gue was the one nine seven three horder litel w\n",
      "bo et or malf in rays patief willed were mulcaph by one nine of the bok a s or pr\n",
      "================================================================================\n",
      "Validation set perplexity: 7.30\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=problem_2B_graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "    \n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches   = train_batches.next()\n",
    "    feed_dict = {train_data[i]:batches[i] for i in range(num_unrollings + 1)}\n",
    "    \n",
    "    _, l, predictions, lr = session.run(\n",
    "        [optimizer, loss, train_prediction, learning_rate], \n",
    "        feed_dict=feed_dict\n",
    "    )\n",
    "    \n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      \n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "        \n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[2:])\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "    \n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):    \n",
    "          feed = collections.deque(maxlen=2)\n",
    "          for _ in range(2):\n",
    "            f = sample(random_distribution())\n",
    "            f = np.argmax(f,1)\n",
    "            feed.append(f)\n",
    "            \n",
    "          sentence = id2characters(feed[0])[0] + id2characters(feed[1])[0]\n",
    "          reset_sample_state.run()\n",
    "        \n",
    "          for _ in range(79): \n",
    "            prediction = sample_prediction.eval({\n",
    "                sample_input[0]:feed[0], \n",
    "                sample_input[1]:feed[1]\n",
    "            })\n",
    "            f          = sample(prediction)\n",
    "            f          = np.argmax(f,1)\n",
    "            feed.append(f)\n",
    "            sentence += id2characters(feed[1])[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "        \n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b             = valid_batches.next()\n",
    "        predictions   = sample_prediction.eval({\n",
    "            sample_input[0]: b[0],\n",
    "            sample_input[1]: b[1]\n",
    "        })\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "        \n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
